{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wll6SDmDjCz",
        "outputId": "f696cbcc-50f2-4cb9-c561-6e49d1b936f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FXU_-lJDl5O",
        "outputId": "63bd00af-5ccd-4c76-c633-41826dd23e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/ai 시스템/dataset/preprocessed/all_images.zip, /content/drive/MyDrive/ai 시스템/dataset/preprocessed/all_images.zip.zip or /content/drive/MyDrive/ai 시스템/dataset/preprocessed/all_images.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "#! unzip -q '/content/drive/MyDrive/ai 시스템/dataset/preprocessed/all_images.zip' -d '/content/drive/MyDrive/ai 시스템/preprocessed'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XIDn5_wDE2Ek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0abb6d7-ddae-4d2f-a83c-5063c2b555a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of files in '/content/drive/MyDrive/ai 시스템/preprocessed/all_images': 54486\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def count_files_in_directory(directory):\n",
        "    # 파일 개수를 세기 위한 초기값 설정\n",
        "    file_count = 0\n",
        "\n",
        "    # 디렉토리 안의 파일 및 폴더 목록 가져오기\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        # 파일 목록의 길이를 파일 개수에 더함\n",
        "        file_count += len(files)\n",
        "\n",
        "    return file_count\n",
        "\n",
        "# 사용할 폴더 경로를 지정\n",
        "directory_path = '/content/drive/MyDrive/ai 시스템/preprocessed/all_images'\n",
        "\n",
        "# 함수 호출 및 결과 출력\n",
        "print(f\"Total number of files in '{directory_path}': {count_files_in_directory(directory_path)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KODALLE"
      ],
      "metadata": {
        "id": "y6-5iNjeWffB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wandb\n",
        "! wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7MQhxxCVcQv",
        "outputId": "dfb5f5f8-481b-4919-d154-6d16b5a023d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.1-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.5.1-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.5.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install adamp\n",
        "! pip install loader\n",
        "! pip install dalle_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Kpd_8FEmWj3L",
        "outputId": "287a3dfc-94cc-4c25-893c-11acd2b99843"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adamp\n",
            "  Downloading adamp-0.3.0.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: adamp\n",
            "  Building wheel for adamp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adamp: filename=adamp-0.3.0-py3-none-any.whl size=5980 sha256=badce1f94dd5481b5e1a578cac806efa8081a3f2c7b107fd847fb029ea92a69e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/ad/0f/b41b1c45b18c66e5eef5d2254415af8055c7e2b0934145157d\n",
            "Successfully built adamp\n",
            "Installing collected packages: adamp\n",
            "Successfully installed adamp-0.3.0\n",
            "Collecting loader\n",
            "  Downloading loader-2017.9.11-py3-none-any.whl (5.9 kB)\n",
            "Collecting bs4 (from loader)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->loader) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->loader) (2.5)\n",
            "Installing collected packages: bs4, loader\n",
            "Successfully installed bs4-0.0.2 loader-2017.9.11\n",
            "Collecting dalle_pytorch\n",
            "  Downloading dalle_pytorch-1.6.6-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting axial-positional-embedding (from dalle_pytorch)\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting DALL-E (from dalle_pytorch)\n",
            "  Downloading DALL_E-0.1-py3-none-any.whl (6.0 kB)\n",
            "Collecting einops>=0.3.2 (from dalle_pytorch)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy (from dalle_pytorch)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from dalle_pytorch) (24.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from dalle_pytorch) (9.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from dalle_pytorch) (2024.5.15)\n",
            "Collecting rotary-embedding-torch (from dalle_pytorch)\n",
            "  Downloading rotary_embedding_torch-0.6.2-py3-none-any.whl (5.3 kB)\n",
            "Collecting taming-transformers-rom1504 (from dalle_pytorch)\n",
            "  Downloading taming_transformers_rom1504-0.0.6-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from dalle_pytorch) (0.19.1)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from dalle_pytorch) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from dalle_pytorch) (0.18.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from dalle_pytorch) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dalle_pytorch) (4.66.4)\n",
            "Collecting youtokentome (from dalle_pytorch)\n",
            "  Downloading youtokentome-1.0.6.tar.gz (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting WebDataset (from dalle_pytorch)\n",
            "  Downloading webdataset-0.2.86-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->dalle_pytorch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->dalle_pytorch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->dalle_pytorch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->dalle_pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->dalle_pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->dalle_pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6->dalle_pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6->dalle_pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6->dalle_pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6->dalle_pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6->dalle_pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6->dalle_pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6->dalle_pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6->dalle_pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6->dalle_pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.6->dalle_pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6->dalle_pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->dalle_pytorch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6->dalle_pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blobfile (from DALL-E->dalle_pytorch)\n",
            "  Downloading blobfile-2.1.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy (from DALL-E->dalle_pytorch)\n",
            "  Downloading mypy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from DALL-E->dalle_pytorch) (1.25.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from DALL-E->dalle_pytorch) (7.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from DALL-E->dalle_pytorch) (2.31.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->dalle_pytorch) (0.2.13)\n",
            "Collecting beartype (from rotary-embedding-torch->dalle_pytorch)\n",
            "  Downloading beartype-0.18.5-py3-none-any.whl (917 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.8/917.8 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf>=2.0.0 (from taming-transformers-rom1504->dalle_pytorch)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning>=1.0.8 (from taming-transformers-rom1504->dalle_pytorch)\n",
            "  Downloading pytorch_lightning-2.2.5-py3-none-any.whl (802 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.3/802.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->dalle_pytorch) (0.23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->dalle_pytorch) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->dalle_pytorch) (0.4.3)\n",
            "Collecting braceexpand (from WebDataset->dalle_pytorch)\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.10/dist-packages (from youtokentome->dalle_pytorch) (8.1.7)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0.0->taming-transformers-rom1504->dalle_pytorch)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle_pytorch)\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities>=0.8.0 (from pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle_pytorch)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Collecting pycryptodomex~=3.8 (from blobfile->DALL-E->dalle_pytorch)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile->DALL-E->dalle_pytorch) (2.0.7)\n",
            "Requirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile->DALL-E->dalle_pytorch) (4.9.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->dalle_pytorch) (2.1.5)\n",
            "Collecting mypy-extensions>=1.0.0 (from mypy->DALL-E->dalle_pytorch)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy->DALL-E->dalle_pytorch) (2.0.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->DALL-E->dalle_pytorch) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->DALL-E->dalle_pytorch) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->DALL-E->dalle_pytorch) (1.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->DALL-E->dalle_pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->DALL-E->dalle_pytorch) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->DALL-E->dalle_pytorch) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->dalle_pytorch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec->torch>=1.6->dalle_pytorch) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning>=1.0.8->taming-transformers-rom1504->dalle_pytorch) (67.7.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch>=1.6->dalle_pytorch) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch>=1.6->dalle_pytorch) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch>=1.6->dalle_pytorch) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch>=1.6->dalle_pytorch) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch>=1.6->dalle_pytorch) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch>=1.6->dalle_pytorch) (4.0.3)\n",
            "Building wheels for collected packages: axial-positional-embedding, youtokentome, antlr4-python3-runtime\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2882 sha256=71c4aaf55d3c14763aaab03c8e7873e8000e2df679cb81820ef21954dc77da0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/cb/39/7ce7ff2d2fd37cfe1fe7b3a3c43cf410632b2ad3b3f3986d73\n",
            "  Building wheel for youtokentome (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for youtokentome: filename=youtokentome-1.0.6-cp310-cp310-linux_x86_64.whl size=1951498 sha256=feaee0fc2c6b847652e7ab502e53cc8371fdd9cb6c817ff94f22b1130104495f\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/85/f8/301d2ba45f43f30bed2fe413efa760bc726b8b660ed9c2900c\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=20a0cff31cc6377b1867d524a3c109beccbe2357e46c19d81fa74ca784da057a\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built axial-positional-embedding youtokentome antlr4-python3-runtime\n",
            "Installing collected packages: braceexpand, antlr4-python3-runtime, youtokentome, WebDataset, pycryptodomex, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, lightning-utilities, ftfy, einops, beartype, nvidia-cusparse-cu12, nvidia-cudnn-cu12, mypy, blobfile, nvidia-cusolver-cu12, torchmetrics, rotary-embedding-torch, axial-positional-embedding, pytorch-lightning, DALL-E, taming-transformers-rom1504, dalle_pytorch\n",
            "Successfully installed DALL-E-0.1 WebDataset-0.2.86 antlr4-python3-runtime-4.9.3 axial-positional-embedding-0.2.1 beartype-0.18.5 blobfile-2.1.1 braceexpand-0.1.7 dalle_pytorch-1.6.6 einops-0.8.0 ftfy-6.2.0 lightning-utilities-0.11.2 mypy-1.10.0 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 pycryptodomex-3.20.0 pytorch-lightning-2.2.5 rotary-embedding-torch-0.6.2 taming-transformers-rom1504-0.0.6 torchmetrics-1.4.0.post0 youtokentome-1.0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "293c4926aff74b1983a63764ba49bf29"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "X0Jb6T7DWpsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python '/content/drive/MyDrive/ai 시스템/KoDALLE/train.py' --wandb_name \"Kodalle\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLRUG3HLWpdc",
        "outputId": "a111c34d-3108-4e48-bad9-d3dfad841ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
            "VQLPIPSWithDiscriminator running with hinge loss.\n",
            "Restored from /content/drive/MyDrive/ai 시스템/KoDALLE/epoch=000007.ckpt\n",
            "Loaded VQGAN from /content/drive/MyDrive/ai 시스템/KoDALLE/epoch=000007.ckpt and /content/drive/MyDrive/ai 시스템/KoDALLE/loaded_VQGAN_blue.yaml\n",
            "full\n",
            "1024 16 16\n",
            "full\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mannie2675\u001b[0m (\u001b[33mannie26751\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20240611_184408-crqzd8pq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mKodalle\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle/runs/crqzd8pq\u001b[0m\n",
            "0 0 loss - 9.941685676574707\n",
            "2024-06-11 18:45:42.118599: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-11 18:45:42.118656: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-11 18:45:42.233651: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-11 18:45:44.664267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "1 0 loss - 3.8074951171875\n",
            "2 0 loss - 3.098296642303467\n",
            "3 0 loss - 2.663395643234253\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch ▁▃▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  iter ▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss █▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  iter 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss 2.6634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mKodalle\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle/runs/crqzd8pq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 4 media file(s), 4 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240611_184408-crqzd8pq/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# epoch = 4, barch_size = 24\n",
        "! python '/content/drive/MyDrive/ai 시스템/KoDALLE/train.py' --wandb_name \"Kodalle\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPKy_iuClUNq",
        "outputId": "d4e094c0-ee04-4e69-8527-c6ea836966a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
            "VQLPIPSWithDiscriminator running with hinge loss.\n",
            "Restored from /content/drive/MyDrive/ai 시스템/KoDALLE/epoch=000007.ckpt\n",
            "Loaded VQGAN from /content/drive/MyDrive/ai 시스템/KoDALLE/epoch=000007.ckpt and /content/drive/MyDrive/ai 시스템/KoDALLE/loaded_VQGAN_blue.yaml\n",
            "full\n",
            "<loader.TextImageDataset object at 0x7d179399bf40>\n",
            "1024 16 16\n",
            "full\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mannie2675\u001b[0m (\u001b[33mannie26751\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20240612_143705-d9fjf21k\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mKodalle\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle/runs/d9fjf21k\u001b[0m\n",
            "0 0 loss - 9.952756881713867\n",
            "2024-06-12 14:37:49.898935: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-12 14:37:49.899014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-12 14:37:50.015638: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-12 14:37:56.683341: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "0 20 loss - 4.726798057556152\n",
            "0 40 loss - 4.378009796142578\n",
            "0 60 loss - 3.948068618774414\n",
            "1 0 loss - 3.915064573287964\n",
            "1 20 loss - 3.707751512527466\n",
            "1 40 loss - 3.477161407470703\n",
            "1 60 loss - 3.2220005989074707\n",
            "2 0 loss - 3.1179800033569336\n",
            "2 20 loss - 3.1702585220336914\n",
            "2 40 loss - 3.0158183574676514\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/ai 시스템/KoDALLE/train.py\", line 212, in <module>\n",
            "    train()\n",
            "  File \"/content/drive/MyDrive/ai 시스템/KoDALLE/train.py\", line 38, in train\n",
            "    loss = dalle(text, images, mask=mask, return_loss=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/ai 시스템/KoDALLE/dalle/models.py\", line 249, in forward\n",
            "    out = self.transformer(tokens)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dalle_pytorch/transformer.py\", line 332, in forward\n",
            "    return self.layers(x, rotary_pos_emb = self.pos_emb, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dalle_pytorch/reversible.py\", line 156, in forward\n",
            "    out =  _ReversibleFunction.apply(x, blocks, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 553, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dalle_pytorch/reversible.py\", line 113, in forward\n",
            "    x = block(x, **kwarg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dalle_pytorch/reversible.py\", line 65, in forward\n",
            "    y1 = x1 + self.f(x2, record_rng=self.training, **f_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dalle_pytorch/reversible.py\", line 40, in forward\n",
            "    return self.net(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dalle_pytorch/transformer.py\", line 88, in forward\n",
            "    return self.fn(x, **kwargs) * self.scale\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dalle_pytorch/transformer.py\", line 101, in forward\n",
            "    x = self.fn(x, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dalle_pytorch/transformer.py\", line 71, in forward\n",
            "    return self.fn(x, cache=cache, cache_key=self.cache_key, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dalle_pytorch/transformer.py\", line 200, in forward\n",
            "    return self.fn(x, cache=cache, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dalle_pytorch/transformer.py\", line 71, in forward\n",
            "    return self.fn(x, cache=cache, cache_key=self.cache_key, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dalle_pytorch/attention.py\", line 67, in forward\n",
            "    q, k, v = apply_pos_emb(rotary_pos_emb[..., offset:, :], (q, k, v))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dalle_pytorch/attention.py\", line 35, in apply_pos_emb\n",
            "    return tuple(map(lambda t: apply_rotary_emb(pos_emb, t), qkv))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dalle_pytorch/attention.py\", line 35, in <lambda>\n",
            "    return tuple(map(lambda t: apply_rotary_emb(pos_emb, t), qkv))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 16, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/rotary_embedding_torch/rotary_embedding_torch.py\", line 49, in apply_rotary_emb\n",
            "    t = (t * freqs.cos() * scale) + (rotate_half(t) * freqs.sin() * scale)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# epoch = 30, barch_size = 16\n",
        "! python '/content/drive/MyDrive/ai 시스템/KoDALLE/train.py' --wandb_name \"Kodalle_30\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2cba502-0a70-4a88-d0a0-0cc9132835ef",
        "id": "5a6fbLMuB2BW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
            "VQLPIPSWithDiscriminator running with hinge loss.\n",
            "Restored from /content/drive/MyDrive/ai 시스템/KoDALLE/epoch=000007.ckpt\n",
            "Loaded VQGAN from /content/drive/MyDrive/ai 시스템/KoDALLE/epoch=000007.ckpt and /content/drive/MyDrive/ai 시스템/KoDALLE/loaded_VQGAN_blue.yaml\n",
            "full\n",
            "<loader.TextImageDataset object at 0x7c951cc8bfd0>\n",
            "1024 16 16\n",
            "full\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mannie2675\u001b[0m (\u001b[33mannie26751\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20240612_093435-8uf8mno7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mKodalle_30\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle/runs/8uf8mno7\u001b[0m\n",
            "0 0 loss - 9.93525505065918\n",
            "2024-06-12 09:34:55.967978: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-12 09:34:55.968056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-12 09:34:56.116646: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-12 09:34:56.389721: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-12 09:35:06.150747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "1 0 loss - 3.866851329803467\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/ai 시스템/KoDALLE/train.py\", line 212, in <module>\n",
            "    train()\n",
            "  File \"/content/drive/MyDrive/ai 시스템/KoDALLE/train.py\", line 43, in train\n",
            "    opt.step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\", line 391, in wrapper\n",
            "    out = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/adamp/adamp.py\", line 91, in step\n",
            "    perturb, wd_ratio = self._projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/adamp/adamp.py\", line 37, in _projection\n",
            "    cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/adamp/adamp.py\", line 30, in _cosine_similarity\n",
            "    return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  iter ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  iter 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss 3.86685\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mKodalle_30\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle/runs/8uf8mno7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 1 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240612_093435-8uf8mno7/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# epoch = 20, barch_size = 16\n",
        "! python '/content/drive/MyDrive/ai 시스템/KoDALLE/train.py' --wandb_name \"Kodalle_20\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae6728b-b754-468a-a2da-bd5cb81cf7f7",
        "id": "zLhLqQAQr701"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_config.json: 100% 375/375 [00:00<00:00, 1.63MB/s]\n",
            "vocab.txt: 100% 248k/248k [00:00<00:00, 1.50MB/s]\n",
            "tokenizer.json: 100% 752k/752k [00:00<00:00, 3.33MB/s]\n",
            "special_tokens_map.json: 100% 173/173 [00:00<00:00, 735kB/s]\n",
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100% 528M/528M [00:03<00:00, 160MB/s]\n",
            "Downloading vgg_lpips model from https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1 to taming/modules/autoencoder/lpips/vgg.pth\n",
            "8.19kB [00:00, 395kB/s]        \n",
            "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
            "VQLPIPSWithDiscriminator running with hinge loss.\n",
            "Restored from /content/drive/MyDrive/ai 시스템/KoDALLE/epoch=000007.ckpt\n",
            "Loaded VQGAN from /content/drive/MyDrive/ai 시스템/KoDALLE/epoch=000007.ckpt and /content/drive/MyDrive/ai 시스템/KoDALLE/loaded_VQGAN_blue.yaml\n",
            "full\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/ai 시스템/KoDALLE/train.py\", line 173, in <module>\n",
            "    dataset_visual = ImgDatasetExample(\n",
            "  File \"/content/drive/MyDrive/ai 시스템/KoDALLE/loader.py\", line 133, in __init__\n",
            "    self.image_files = [\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 1034, in glob\n",
            "    for p in selector.select_from(self):\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 493, in _select_from\n",
            "    for p in successor_select(starting_point, is_dir, exists, scandir):\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 440, in _select_from\n",
            "    with scandir(parent_path) as scandir_it:\n",
            "OSError: [Errno 5] Input/output error: '/content/drive/MyDrive/ai 시스템/preprocessed/all_images'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# epoch = 5, batch_size = 16\n",
        "! python '/content/drive/MyDrive/ai 시스템/KoDALLE/train.py' --wandb_name \"Kodalle_5\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ni1vFgbG5bo",
        "outputId": "cfeab21a-d0b7-44c4-dc3f-b66bbe6a0aa5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
            "VQLPIPSWithDiscriminator running with hinge loss.\n",
            "Restored from /content/drive/MyDrive/ai 시스템/KoDALLE/epoch=000007.ckpt\n",
            "Loaded VQGAN from /content/drive/MyDrive/ai 시스템/KoDALLE/epoch=000007.ckpt and /content/drive/MyDrive/ai 시스템/KoDALLE/loaded_VQGAN_blue.yaml\n",
            "full\n",
            "<loader.TextImageDataset object at 0x7835a6252bf0>\n",
            "1024 16 16\n",
            "full\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mannie2675\u001b[0m (\u001b[33mannie26751\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20240612_161743-7ics3frh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mKodalle_5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle/runs/7ics3frh\u001b[0m\n",
            "0 0 loss - 9.947244644165039\n",
            "2024-06-12 16:17:58.977231: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-12 16:17:58.977301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-12 16:17:59.097346: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-12 16:17:59.325929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-12 16:18:10.390575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "0 20 loss - 4.6909379959106445\n",
            "0 40 loss - 4.322117328643799\n",
            "0 60 loss - 3.993182420730591\n",
            "1 0 loss - 3.883690595626831\n",
            "1 20 loss - 3.6750950813293457\n",
            "1 40 loss - 3.500836133956909\n",
            "1 60 loss - 3.239720106124878\n",
            "2 0 loss - 3.0891551971435547\n",
            "2 20 loss - 2.9327285289764404\n",
            "2 40 loss - 2.92680287361145\n",
            "2 60 loss - 2.8224544525146484\n",
            "3 0 loss - 2.578993797302246\n",
            "3 20 loss - 2.499619722366333\n",
            "3 40 loss - 2.6003823280334473\n",
            "3 60 loss - 2.5117835998535156\n",
            "4 0 loss - 2.258887529373169\n",
            "4 20 loss - 2.1794657707214355\n",
            "4 40 loss - 2.3197033405303955\n",
            "4 60 loss - 2.2995362281799316\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch ▁▁▁▁▃▃▃▃▅▅▅▅▆▆▆▆████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  iter ▁▃▆█▁▃▆█▁▃▆█▁▃▆█▁▃▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss █▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  iter 60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss 2.29954\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mKodalle_5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle/runs/7ics3frh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/annie26751/Kodalle\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 10 media file(s), 5 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240612_161743-7ics3frh/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "4w_lsuV0cqEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqHzrnatea60",
        "outputId": "8e0e47b3-a5ff-4f92-d812-47a4e44679e1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting utils\n",
            "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: utils\n",
            "  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13906 sha256=135959a0c188fe15d0de7c6d168c4caf469f244d0f562e03553aa8a37a0c55cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/39/f5/9d0ca31dba85773ececf0a7f5469f18810e1c8a8ed9da28ca7\n",
            "Successfully built utils\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from einops import repeat\n",
        "from axial_positional_embedding import AxialPositionalEmbedding\n",
        "from einops import rearrange\n",
        "\n",
        "from dalle_pytorch import DiscreteVAE\n",
        "from dalle_pytorch.vae import OpenAIDiscreteVAE, VQGanVAE\n",
        "\n",
        "from dalle_pytorch.transformer import Transformer, DivideMax\n",
        "from utils import *\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DALLE_Klue_Roberta(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        # dim,\n",
        "        vae,\n",
        "        num_text_tokens=10000,\n",
        "        text_seq_len=256,\n",
        "        depth,\n",
        "        heads=8,\n",
        "        dim_head=64,\n",
        "        reversible=False,\n",
        "        attn_dropout=0.0,\n",
        "        ff_dropout=0,\n",
        "        sparse_attn=False,\n",
        "        attn_types=None,\n",
        "        loss_img_weight=7,\n",
        "        stable=False,\n",
        "        sandwich_norm=False,\n",
        "        shift_tokens=True,\n",
        "        rotary_emb=False,\n",
        "        wte_dir=None,\n",
        "        wpe_dir=None,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "        assert isinstance(\n",
        "            vae, (DiscreteVAE, OpenAIDiscreteVAE, VQGanVAE)\n",
        "        ), \"vae must be an instance of DiscreteVAE\"\n",
        "        image_size = vae.image_size\n",
        "        num_image_tokens = vae.num_tokens\n",
        "        image_fmap_size = vae.image_size // (2 ** vae.num_layers)\n",
        "        image_seq_len = image_fmap_size ** 2\n",
        "\n",
        "        num_text_tokens = (\n",
        "            num_text_tokens + text_seq_len\n",
        "        )  # reserve unique padding tokens for each position (text seq len)\n",
        "\n",
        "        def always(value):\n",
        "          return lambda *args, **kwargs: value\n",
        "\n",
        "        self.text_emb = torch.load(wte_dir)\n",
        "        dim = self.text_emb.weight.shape[1]\n",
        "        self.image_emb = nn.Embedding(num_image_tokens, dim)\n",
        "        print(dim, image_fmap_size, image_fmap_size)\n",
        "        self.text_pos_emb = (\n",
        "            torch.load(wpe_dir) if not rotary_emb else always(0)\n",
        "        )  # +1 for <bos>\n",
        "        self.image_pos_emb = (\n",
        "            AxialPositionalEmbedding(\n",
        "                dim, axial_shape=(image_fmap_size, image_fmap_size)\n",
        "            )\n",
        "            if not rotary_emb\n",
        "            else always(0)\n",
        "        )\n",
        "\n",
        "        self.num_text_tokens = num_text_tokens  # for offsetting logits index and calculating cross entropy loss\n",
        "        self.num_image_tokens = num_image_tokens\n",
        "\n",
        "        self.text_seq_len = text_seq_len\n",
        "        self.image_seq_len = image_seq_len\n",
        "\n",
        "        seq_len = text_seq_len + image_seq_len\n",
        "        total_tokens = num_text_tokens + num_image_tokens\n",
        "        self.total_tokens = total_tokens\n",
        "        self.total_seq_len = seq_len\n",
        "\n",
        "        self.vae = vae\n",
        "\n",
        "        def set_requires_grad(model, value):\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = value\n",
        "        set_requires_grad(self.vae, False)  # freeze VAE from being trained\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            dim=dim,\n",
        "            causal=True,\n",
        "            seq_len=seq_len,\n",
        "            depth=depth,\n",
        "            heads=heads,\n",
        "            dim_head=dim_head,\n",
        "            reversible=reversible,\n",
        "            attn_dropout=attn_dropout,\n",
        "            ff_dropout=ff_dropout,\n",
        "            attn_types=attn_types,\n",
        "            image_fmap_size=image_fmap_size,\n",
        "            sparse_attn=sparse_attn,\n",
        "            stable=stable,\n",
        "            sandwich_norm=sandwich_norm,\n",
        "            shift_tokens=shift_tokens,\n",
        "            rotary_emb=rotary_emb,\n",
        "        )\n",
        "\n",
        "        self.stable = stable\n",
        "\n",
        "        if stable:\n",
        "            self.norm_by_max = DivideMax(dim=-1)\n",
        "\n",
        "        self.to_logits = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, self.total_tokens),\n",
        "        )\n",
        "\n",
        "        seq_range = torch.arange(seq_len)\n",
        "        logits_range = torch.arange(total_tokens)\n",
        "\n",
        "        seq_range = rearrange(seq_range, \"n -> () n ()\")\n",
        "        logits_range = rearrange(logits_range, \"d -> () () d\")\n",
        "\n",
        "        logits_mask = (\n",
        "            (seq_range >= text_seq_len) & (logits_range < num_text_tokens)\n",
        "        ) | ((seq_range < text_seq_len) & (logits_range >= num_text_tokens))\n",
        "\n",
        "        self.register_buffer(\"logits_mask\", logits_mask, persistent=False)\n",
        "        self.loss_img_weight = loss_img_weight\n",
        "\n",
        "    @torch.no_grad()\n",
        "\n",
        "    def exists(obj):\n",
        "            return obj is not None\n",
        "\n",
        "    #eval_decorator\n",
        "    def generate_images(\n",
        "        self,\n",
        "        encoded_text,\n",
        "        *,\n",
        "        clip=None,\n",
        "        filter_thres=0.5,\n",
        "        temperature=1.0,\n",
        "        img=None,\n",
        "        num_init_img_tokens=None,\n",
        "        img_num=1,\n",
        "    ):\n",
        "        text = encoded_text['input_ids']\n",
        "        text=repeat(text,'() n -> b n',b=img_num)\n",
        "        mask=encoded_text['attention_mask']\n",
        "        vae, text_seq_len, image_seq_len, num_text_tokens = (\n",
        "            self.vae,\n",
        "            self.text_seq_len,\n",
        "            self.image_seq_len,\n",
        "            self.num_text_tokens,\n",
        "        )\n",
        "        total_len = text_seq_len + image_seq_len\n",
        "\n",
        "        text = text[:, :text_seq_len]  # make sure text is within bounds\n",
        "        out = text\n",
        "\n",
        "\n",
        "        if exists(img):\n",
        "            image_size = vae.image_size\n",
        "            assert (\n",
        "                img.shape[1] == 3\n",
        "                and img.shape[2] == image_size\n",
        "                and img.shape[3] == image_size\n",
        "                ), f\"input image must have the correct image size {image_size}\"\n",
        "\n",
        "            indices = vae.get_codebook_indices(img)\n",
        "            num_img_tokens = default(\n",
        "                num_init_img_tokens, int(0.4375 * image_seq_len)\n",
        "            )  # OpenAI used 14 * 32 initial tokens to prime\n",
        "            assert (\n",
        "                num_img_tokens < image_seq_len\n",
        "            ), \"number of initial image tokens for priming must be less than the total image token sequence length\"\n",
        "\n",
        "            indices = indices[:, :num_img_tokens]\n",
        "            print(\"out shape:\", out.shape)\n",
        "            print(\"sample shape:\", sample.shape)\n",
        "            out = torch.cat((out, indices), dim=-1)\n",
        "\n",
        "        for cur_len in tqdm(range(out.shape[1], total_len)):\n",
        "          is_image = cur_len >= text_seq_len\n",
        "\n",
        "          text, image = out[:, :text_seq_len], out[:, text_seq_len:]\n",
        "\n",
        "          logits = self(text, image, mask=mask)[:, -1, :]\n",
        "\n",
        "          filtered_logits = top_k(logits, thres=filter_thres)\n",
        "          probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
        "          sample = torch.multinomial(probs, 1)\n",
        "\n",
        "          # 여기에서 차원을 조정합니다.\n",
        "          if sample.dim() == 1:\n",
        "              sample = sample.unsqueeze(1)\n",
        "\n",
        "          # 차원이 조정된 텐서를 연결합니다.\n",
        "          sample -= (num_text_tokens if is_image else 0)  # 필요한 조정\n",
        "          out = torch.cat((out, sample), dim=-1)\n",
        "\n",
        "          if out.shape[1] <= text_seqlen:\n",
        "              mask = F.pad(mask, (0, 1), value=True)\n",
        "\n",
        "\n",
        "          img_seq = out[:, -image_seq_len:]\n",
        "          images = vae.decode(img_seq)\n",
        "\n",
        "        if exists(clip):\n",
        "            #encoded_text = encoded_text.to(\"cuda\")\n",
        "            text_embeds, image_embeds = clip(encoded_text, images)\n",
        "            logits = text_embeds @ image_embeds.T\n",
        "            return images, logits\n",
        "\n",
        "        return images\n",
        "    def forward(self, text, image=None, mask=None, return_loss=False):\n",
        "        assert (\n",
        "            text.shape[-1] == self.text_seq_len\n",
        "        ), f\"the length {text.shape[-1]} of the text tokens you passed in does not have the correct length ({self.text_seq_len})\"\n",
        "        device, total_seq_len = text.device, self.total_seq_len\n",
        "\n",
        "        # make sure padding in text tokens get unique padding token id\n",
        "        text = F.pad(text, (1, 0), value=0)\n",
        "\n",
        "        tokens = self.text_emb(text)\n",
        "        tokens += self.text_pos_emb(torch.arange(text.shape[1], device=device))\n",
        "\n",
        "        seq_len = tokens.shape[1]\n",
        "\n",
        "        if exists(image) and not is_empty(image):\n",
        "            is_raw_image = len(image.shape) == 4\n",
        "\n",
        "            if is_raw_image:\n",
        "                image_size = self.vae.image_size\n",
        "                assert tuple(image.shape[1:]) == (\n",
        "                    3,\n",
        "                    image_size,\n",
        "                    image_size,\n",
        "                ), f\"invalid image of dimensions {image.shape} passed in during training\"\n",
        "\n",
        "                image = self.vae.get_codebook_indices(image)\n",
        "            image_len = image.shape[1]\n",
        "            image_emb = self.image_emb(image)\n",
        "            image_emb += self.image_pos_emb(image_emb)\n",
        "\n",
        "            tokens = torch.cat((tokens, image_emb), dim=1)\n",
        "\n",
        "            seq_len += image_len\n",
        "\n",
        "        # when training, if the length exceeds the total text + image length\n",
        "        # remove the last token, since it needs not to be trained\n",
        "\n",
        "        if tokens.shape[1] > total_seq_len:\n",
        "            seq_len -= 1\n",
        "            tokens = tokens[:, :-1]\n",
        "\n",
        "        if self.stable:\n",
        "            alpha = 0.1\n",
        "            tokens = tokens * alpha + tokens.detach() * (1 - alpha)\n",
        "\n",
        "        out = self.transformer(tokens)\n",
        "\n",
        "        if self.stable:\n",
        "            out = self.norm_by_max(out)\n",
        "\n",
        "        logits = self.to_logits(out)\n",
        "\n",
        "        # mask logits to make sure text predicts text (except last token), and image predicts image\n",
        "\n",
        "        logits_mask = self.logits_mask[:, :seq_len]\n",
        "        max_neg_value = -torch.finfo(logits.dtype).max\n",
        "        logits.masked_fill_(logits_mask, max_neg_value)\n",
        "\n",
        "        if not return_loss:\n",
        "            return logits\n",
        "\n",
        "        assert exists(image), \"when training, image must be supplied\"\n",
        "\n",
        "        offsetted_image = image + self.num_text_tokens\n",
        "        labels = torch.cat((text[:, 1:], offsetted_image), dim=1)\n",
        "\n",
        "        logits = rearrange(logits, \"b n c -> b c n\")\n",
        "\n",
        "        loss_text = F.cross_entropy(\n",
        "            logits[:, :, : self.text_seq_len], labels[:, : self.text_seq_len]\n",
        "        )\n",
        "        loss_img = F.cross_entropy(\n",
        "            logits[:, :, self.text_seq_len :], labels[:, self.text_seq_len :]\n",
        "        )\n",
        "\n",
        "        loss = (loss_text + self.loss_img_weight * loss_img) / (\n",
        "            self.loss_img_weight + 1\n",
        "        )\n",
        "        return loss"
      ],
      "metadata": {
        "id": "XtxFnIKjdEtE"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from dalle_pytorch import VQGanVAE\n",
        "#from dalle.models import DALLE_Klue_Roberta\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import yaml\n",
        "from easydict import EasyDict\n",
        "\n",
        "\n",
        "\n",
        "dalle_config_path = '/content/drive/MyDrive/ai 시스템/KoDALLE/configs/dalle_config.yaml'\n",
        "dalle_path = '/content/drive/MyDrive/ai 시스템/KoDALLE/dalle_uk.pt'\n",
        "\n",
        "vqgan_config_path = '/content/drive/MyDrive/ai 시스템/KoDALLE/loaded_VQGAN_blue.yaml'\n",
        "vqgan_path = '/content/drive/MyDrive/ai 시스템/KoDALLE/epoch=000007.ckpt'\n",
        "\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFEj9kbqcrEA",
        "outputId": "f2ec717f-d1c9-479c-bf02-e5af13e32cfb"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
        "\n",
        "with open(dalle_config_path, \"r\") as f:\n",
        "    dalle_config = yaml.load(f, Loader=yaml.Loader)\n",
        "    DALLE_CFG = EasyDict(dalle_config[\"DALLE_CFG\"])\n",
        "\n",
        "DALLE_CFG.VOCAB_SIZE = tokenizer.vocab_size\n",
        "\n",
        "vae = VQGanVAE(\n",
        "    vqgan_model_path=vqgan_path,\n",
        "    vqgan_config_path=vqgan_config_path\n",
        ")\n",
        "\n",
        "DALLE_CFG.IMAGE_SIZE = vae.image_size\n",
        "\n",
        "dalle_params = dict(\n",
        "    num_text_tokens=tokenizer.vocab_size,\n",
        "    text_seq_len=DALLE_CFG.TEXT_SEQ_LEN,\n",
        "    depth=DALLE_CFG.DEPTH,\n",
        "    heads=DALLE_CFG.HEADS,\n",
        "    dim_head=DALLE_CFG.DIM_HEAD,\n",
        "    reversible=DALLE_CFG.REVERSIBLE,\n",
        "    loss_img_weight=DALLE_CFG.LOSS_IMG_WEIGHT,\n",
        "    attn_types=DALLE_CFG.ATTN_TYPES,\n",
        "    ff_dropout=DALLE_CFG.FF_DROPOUT,\n",
        "    attn_dropout=DALLE_CFG.ATTN_DROPOUT,\n",
        "    stable=DALLE_CFG.STABLE,\n",
        "    shift_tokens=DALLE_CFG.SHIFT_TOKENS,\n",
        "    rotary_emb=DALLE_CFG.ROTARY_EMB,\n",
        ")\n",
        "\n",
        "dalle = DALLE_Klue_Roberta(\n",
        "    vae=vae,\n",
        "    wte_dir=\"/content/drive/MyDrive/ai 시스템/KoDALLE/models/roberta_large_wte.pt\",\n",
        "    wpe_dir=\"/content/drive/MyDrive/ai 시스템/KoDALLE/models/roberta_large_wpe.pt\",\n",
        "    **dalle_params\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "loaded_obj = torch.load(dalle_path, map_location=torch.device('cuda:0'))\n",
        "dalle_params, vae_params, weights = loaded_obj['hparams'], loaded_obj['vae_params'], loaded_obj['weights']\n",
        "dalle.load_state_dict(weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOk1Pk_sdBVr",
        "outputId": "af0d16c4-6019-4329-9d8b-b760b33a31f8"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
            "VQLPIPSWithDiscriminator running with hinge loss.\n",
            "Restored from /content/drive/MyDrive/ai 시스템/KoDALLE/epoch=000007.ckpt\n",
            "Loaded VQGAN from /content/drive/MyDrive/ai 시스템/KoDALLE/epoch=000007.ckpt and /content/drive/MyDrive/ai 시스템/KoDALLE/loaded_VQGAN_blue.yaml\n",
            "1024 16 16\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '볼이 넓은 계란형 얼굴이며 앞머리가 이마의 양쪽 끝을 가리고 있어 모양은 보이지 않는다.   오른쪽 턱의 각진 부분이 왼쪽에 비해 아래로 내려와 있고 왼쪽은 약간 완만한 형태이다.  턱끝으로 내려오는 턱모양은 약간 둥근형으로 보인다. 왼쪽의 볼이 더 평평하고 넓은 편이다.'\n",
        "\n",
        "encoded_dict = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=DALLE_CFG.TEXT_SEQ_LEN,\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=True,  # for RoBERTa\n",
        ").to(device)\n",
        "\n",
        "encoded_text = encoded_dict['input_ids']\n",
        "mask = encoded_dict['attention_mask']\n",
        "\n",
        "print(encoded_text)\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9hljLgTfBqk",
        "outputId": "ed885b81-c291-480f-ffe1-910786aee268"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,  1164,  2052,   748,  2073,  8493,  2444,  3977,  2052,  2307,\n",
            "         24819,  2116,  8950,  2079,  8108,   711,  2069,  5246,  2088,  1513,\n",
            "          2051,  4616,  2073,  3783,  2118,  1380,  2259,  2062,    18,  6735,\n",
            "          1778,  2079,   544,  2043,  3884,  2052,  6561,  2170,  4357,  4402,\n",
            "          2200, 13567,  1513,  2088,  6561,  2073,  4943, 18026,  2470,  4337,\n",
            "         28674,    18,  1778,  3141,  6233,  9005,  2259,  1778,  2391,  2221,\n",
            "          2073,  4943, 15207,     2]], device='cuda:0')\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyhlZyIjoyOU",
        "outputId": "0e69dd66-9772-4b3a-fd25-a0fb0ed00503"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj4hKAtco0ZZ",
        "outputId": "fdc6e8ef-7e98-4e32-c59e-5bf738b4f57c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def exists(obj):\n",
        "    return obj is not None\n",
        "\n",
        "def is_empty(obj):\n",
        "  return not obj is not None\n",
        "\n",
        "def top_k(logits, thres):\n",
        "    # Assuming logits is a PyTorch tensor and you want the indices of the top k elements above a threshold\n",
        "    k = 5  # You can dynamically set k based on your requirements\n",
        "    # Return the values and indices where values are greater than a threshold\n",
        "    mask = logits > thres\n",
        "    values, indices = torch.topk(logits[mask], k=min(k, mask.sum()), largest=True, sorted=True)\n",
        "    return values\n",
        "\n",
        "my_vae = DiscreteVAE()\n",
        "my_depth = 12\n",
        "\n",
        "# DALLE_Klue_Roberta 인스턴스 생성\n",
        "dalle = DALLE_Klue_Roberta(vae=my_vae, depth=my_depth)\n",
        "\n",
        "image = dalle.generate_images(\n",
        "    encoded_dict,\n",
        "    #mask=mask,\n",
        "    filter_thres=0.9  # topk sampling at 0.9\n",
        ")\n",
        "\n",
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "YbzXdXD8k7Hl",
        "outputId": "8fa38909-0b40-484a-8c5a-bf22e6aafafb"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'seek'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-c71ee20ec4c7>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# DALLE_Klue_Roberta 인스턴스 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdalle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDALLE_Klue_Roberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_vae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m image = dalle.generate_images(\n",
            "\u001b[0;32m<ipython-input-77-2e342d942be4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vae, num_text_tokens, text_seq_len, depth, heads, dim_head, reversible, attn_dropout, ff_dropout, sparse_attn, attn_types, loss_img_weight, stable, sandwich_norm, shift_tokens, rotary_emb, wte_dir, wpe_dir)\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwte_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_image_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_open_buffer_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m'r'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_open_buffer_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected 'r' or 'w' in mode but got {mode}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, buffer)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0m_check_seekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0mraise_err_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seek\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mraise_err_msg\u001b[0;34m(patterns, e)\u001b[0m\n\u001b[1;32m    534\u001b[0m                                 \u001b[0;34m+\u001b[0m \u001b[0;34m\" Please pre-load the data into a buffer like io.BytesIO and\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m                                 + \" try to load from it instead.\")\n\u001b[0;32m--> 536\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)\n",
        "\n",
        "T.ToPILImage()(image.squeeze())"
      ],
      "metadata": {
        "id": "G5KxfWseftxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_montage(text):\n",
        "    encoded_dict = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=DALLE_CFG.TEXT_SEQ_LEN,\n",
        "        add_special_tokens=True,\n",
        "        return_token_type_ids=True,  # for RoBERTa\n",
        "    ).to(device)\n",
        "\n",
        "    encoded_text = encoded_dict['input_ids']\n",
        "    mask = encoded_dict['attention_mask']\n",
        "\n",
        "    image = dalle.generate_images(\n",
        "        encoded_text,\n",
        "        mask=mask,\n",
        "        filter_thres=0.9  # topk sampling at 0.9\n",
        "    )\n",
        "\n",
        "    return T.ToPILImage()(image.squeeze())"
      ],
      "metadata": {
        "id": "W2SS5omQf0Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = input('text : ')\n",
        "text_to_montage(text)"
      ],
      "metadata": {
        "id": "rLUeEZ0Ff2LL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}